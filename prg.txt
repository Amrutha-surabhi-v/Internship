import cv2
import numpy as np

# Load YOLO
net = cv2.dnn.readNet(yolov3.weights, yolov3.cfg)
classes = []
with open(coco.names, r) as f
    classes = [line.strip() for line in f.readlines()]

# Set up the detection parameters
layer_names = net.getLayerNames()
output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]

# Load and prepare the image
img = cv2.imread(image.jpg)
height, width, channels = img.shape

# Detect objects
blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
net.setInput(blob)
outs = net.forward(output_layers)

# Process detections
class_ids = []
confidences = []
boxes = []

for out in outs
    for detection in out
        scores = detection[5]
        class_id = np.argmax(scores)
        confidence = scores[class_id]
        if confidence  0.5 and classes[class_id] == person
            # Object detected
            center_x = int(detection[0]  width)
            center_y = int(detection[1]  height)
            w = int(detection[2]  width)
            h = int(detection[3]  height)
            
            # Rectangle coordinates
            x = int(center_x - w  2)
            y = int(center_y - h  2)
            
            boxes.append([x, y, w, h])
            confidences.append(float(confidence))
            class_ids.append(class_id)

# Apply non-maximum suppression
indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)

# Draw bounding boxes
for i in range(len(boxes))
    if i in indexes
        x, y, w, h = boxes[i]
        label = str(classes[class_ids[i]])
        confidence = confidences[i]
        color = (0, 255, 0)  # Green
        cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)
        cv2.putText(img, f{label} {confidence.2f}, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

# Display the result
cv2.imshow(Person Detection, img)
cv2.waitKey(0)
cv2.destroyAllWindows()

TO DETECT PERSON OR PICTURE
import cv2
import numpy as np

# Load the face detection classifier
face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")

# Initialize the webcam
video_capture = cv2.VideoCapture(0)

# Initialize variables for motion detection
prev_gray = None
face_motion_detected = False

def detect_bounding_box(vid):
    gray_image = cv2.cvtColor(vid, cv2.COLOR_BGR2GRAY)
    faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))
    return faces

while True:
    # Capture the current frame
    ret, frame = video_capture.read()
    if not ret:
        break

    # Detect faces in the current frame
    faces = detect_bounding_box(frame)

    # Convert the current frame to grayscale
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # If we have a previous frame, detect motion
    if prev_gray is not None:
        face_motion_detected = False

        for (x, y, w, h) in faces:
            # Compute the absolute difference between the current and previous frame
            # but only within the face bounding box
            face_diff = cv2.absdiff(
                prev_gray[y:y+h, x:x+w],
                gray[y:y+h, x:x+w]
            )

            # Threshold the difference
            _, face_thresh = cv2.threshold(face_diff, 25, 255, cv2.THRESH_BINARY)

            # Count non-zero pixels (motion pixels)
            motion_pixels = cv2.countNonZero(face_thresh)

            # If the number of motion pixels is above a threshold, consider it as motion
            if motion_pixels > 50:  # You can adjust this threshold
                face_motion_detected = True

            # Draw rectangle around the face
            color = (0, 255, 0) if face_motion_detected else (0, 0, 255)
            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 4)

        # Display appropriate text based on motion detection
        if face_motion_detected:
            cv2.putText(frame, "person detected", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)
        else:
            cv2.putText(frame, "picture detected", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)

    # Display the frame
    cv2.imshow("Face and Motion Detection", frame)

    # Update the previous frame
    prev_gray = gray.copy()

    # Break loop on 'q' key press
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the webcam and close windows
video_capture.release()
cv2.destroyAllWindows()

TO DRAW BOUND box
import cv2



face_classifier = cv2.CascadeClassifier(
    cv2.data.haarcascades + "haarcascade_frontalface_default.xml"
)
video_capture = cv2.VideoCapture(0)
def detect_bounding_box(vid):
    gray_image = cv2.cvtColor(vid, cv2.COLOR_BGR2GRAY)
    faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))
    for (x, y, w, h) in faces:
        cv2.rectangle(vid, (x, y), (x + w, y + h), (0, 255, 0), 4)
    return faces
while True:

    result, video_frame = video_capture.read()  # read frames from the video
    if result is False:
        break  # terminate the loop if the frame is not read successfully

    faces = detect_bounding_box(
        video_frame
    )  # apply the function we created to the video frame

    cv2.imshow(
        "My Face Detection Project", video_frame
    )  # display the processed frame in a window named "My Face Detection Project"

    if cv2.waitKey(1) & 0xFF == ord("q"):
        break

video_capture.release()
cv2.destroyAllWindows()


MOTION DETECTEOR
import cv2
import numpy as np
import time

# Load the face detection classifier
face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")

# Initialize the webcam
video_capture = cv2.VideoCapture(0)

# Initialize variables for motion detection
prev_gray = None
face_motion_detected = False
last_motion_time = time.time()
last_check_time = time.time()
check_interval = 3  # Check for motion every 3 seconds
picture_threshold = 5  # Declare as picture if no motion for 10 seconds

def detect_bounding_box(vid):
    gray_image = cv2.cvtColor(vid, cv2.COLOR_BGR2GRAY)
    faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))
    return faces

while True:
    # Capture the current frame
    ret, frame = video_capture.read()
    if not ret:
        break

    # Detect faces in the current frame
    faces = detect_bounding_box(frame)

    # Convert the current frame to grayscale
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    current_time = time.time()

    # Check for motion every 3 seconds
    if current_time - last_check_time >= check_interval:
        last_check_time = current_time
        face_motion_detected = False

        if prev_gray is not None and len(faces) > 0:
            for (x, y, w, h) in faces:
                # Compute the absolute difference between the current and previous frame
                # but only within the face bounding box
                face_diff = cv2.absdiff(
                    prev_gray[y:y+h, x:x+w],
                    gray[y:y+h, x:x+w]
                )

                # Threshold the difference
                _, face_thresh = cv2.threshold(face_diff, 25, 255, cv2.THRESH_BINARY)

                # Count non-zero pixels (motion pixels)
                motion_pixels = cv2.countNonZero(face_thresh)

                # If the number of motion pixels is above a threshold, consider it as motion
                if motion_pixels > 50:  # You can adjust this threshold
                    face_motion_detected = True
                    last_motion_time = current_time
                    break

    # Determine if it's a person or picture
    if face_motion_detected or (current_time - last_motion_time < picture_threshold):
        status = "person detected"
        color = (0, 255, 0)  # Green
    else:
        status = "picture detected"
        color = (0, 0, 255)  # Red

    # Draw rectangles around faces and display status
    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 4)

    cv2.putText(frame, status, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 3)

    # Display the frame
    cv2.imshow("Face and Motion Detection", frame)

    # Update the previous frame
    prev_gray = gray.copy()

    # Break loop on 'q' key press
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the webcam and close windows
video_capture.release()
cv2.destroyAllWindows()

